{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gJr_9dXGpJ05",
        "outputId": "d830c59c-5ec4-4f00-d56c-fdc64583accf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Google_Quest\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "best_param_score_uncased_1_1.pt  datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIS9Z-V5eJ17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\"\n",
        "#\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "#https://www.kaggle.com/phoenix9032/pytorch-bert-plain/data\n",
        "#https://huggingface.co/transformers/model_doc/bert.html?highlight=bertmodel#transformers.BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUUFcNA2p1Gv",
        "colab_type": "code",
        "outputId": "a122b105-0e71-43cc-f1d1-44bdf4b15133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Google_Quest\")\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_param_score_uncased_1_1.pt  datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwKNabMfUCe",
        "colab_type": "code",
        "outputId": "9d75f76a-aec8-4ec8-bc72-b5c1e49945fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "## Loading Libraries  -No they are not for books\n",
        "!pip install transformers\n",
        "import transformers, sys, os, gc\n",
        "import numpy as np, pandas as pd, math\n",
        "import torch, random, os, multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import torch, time\n",
        "\n",
        "from datasets.helpers.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "from transformers.modeling_bert import BertPreTrainedModel \n",
        "from tqdm import tqdm\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM-XEXCzfVHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = 'datasets/google-quest-challenge/'\n",
        "## Make results reproducible .Else noone will believe you .\n",
        "import random\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVYZYZ4gBI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipeLineConfig:\n",
        "    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n",
        "        self.lr = lr\n",
        "        self.warmup = warmup\n",
        "        self.accum_steps = accum_steps\n",
        "        self.epochs = epochs\n",
        "        self.seed = seed\n",
        "        self.expname = expname\n",
        "        self.head_tail = head_tail\n",
        "        self.freeze = freeze\n",
        "        self.question_weight = question_weight\n",
        "        self.answer_weight =answer_weight\n",
        "        self.fold = fold\n",
        "        self.train = train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQfbPZngDo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_1 = PipeLineConfig(3e-5,0.05,1,2,42,'uncased_1',True,False,0.7,0.3,5,True)\n",
        "config = config_1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_An6zZgJvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_everything(config.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfV0bbt4gLrY",
        "colab_type": "code",
        "outputId": "370a4971-cc64-4d9a-e7b6-98e1c24bdb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## load the data \n",
        "train = pd.read_csv(ROOT+'train.csv')\n",
        "test = pd.read_csv(ROOT+'test.csv')\n",
        "sub = pd.read_csv(ROOT+'sample_submission.csv')\n",
        "train_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\n",
        "print(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 6079, test size: 476 , sample size: 476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdzOrciygN1M",
        "colab_type": "code",
        "outputId": "a3154a38-4445-4372-94de-3db06cc88aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target_cols = sub.columns.tolist()[1:]\n",
        "len(target_cols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGI-VNrsgdDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the Ref Kernel's\n",
        "from math import floor, ceil\n",
        "\n",
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    \n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "        \n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    \n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length=290, t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    #350+128+30 = 508 + 4 = 512\n",
        "    \n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "        q_len_head = round(q_new_len/2)\n",
        "        q_len_tail = -1* (q_new_len -q_len_head)\n",
        "        a_len_head = round(a_new_len/2)\n",
        "        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n",
        "        t = t[:t_new_len]\n",
        "        if config.head_tail :\n",
        "            q = q[:q_len_head]+q[q_len_tail:]\n",
        "            a = a[:a_len_head]+a[a_len_tail:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "            a = a[:a_new_len] ## No Head+Tail ,usual processing\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "   # stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def _get_stoken_output(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "    return stoken\n",
        "\n",
        "def compute_input_tokens(df, columns, tokenizer, max_sequence_length):\n",
        "    \n",
        "    input_tokens, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
        "        tokens= _get_stoken_output(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_tokens.append(tokens)\n",
        "    return input_tokens\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length,t_max_len=30, q_max_len=128, a_max_len=128):\n",
        "    \n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in df[columns].iterrows():\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length,t_max_len, q_max_len, a_max_len)\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "    return [\n",
        "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
        "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
        "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
        "    ]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqUfn6XgjlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QuestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inputs, lengths, labels = None):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        if labels is not None:\n",
        "            self.labels = labels\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        input_ids       = self.inputs[0][idx]\n",
        "        input_masks     = self.inputs[1][idx]\n",
        "        input_segments  = self.inputs[2][idx]\n",
        "        lengths         = self.lengths[idx]\n",
        "        if self.labels is not None: # targets\n",
        "            labels = self.labels[idx]\n",
        "            return input_ids, input_masks, input_segments, labels, lengths\n",
        "        return input_ids, input_masks, input_segments, lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YG6sXicgnU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomBert(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(CustomBert, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.bn = nn.BatchNorm1d(1024)\n",
        "        self.linear  = nn.Linear(config.hidden_size,1024)\n",
        "        self.classifier = nn.Linear(1024, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        lin_output = F.relu(self.bn(self.linear(pooled_output))) ## Note : This Linear layer is added without expert supervision . This will worsen the results . \n",
        "                                               ## But you are smarter than me , so you will figure out,how to customize better.\n",
        "        lin_output = self.dropout(lin_output)    \n",
        "        logits = self.classifier(lin_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKeoTkDAgu3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test\n",
        "#from transformers import BertModel, BertTokenizer\n",
        "#import torch\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "#input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "#outputs = model(input_ids)\n",
        "\n",
        "#outputs[3][12].shape\n",
        "#len(outputs) #4\n",
        "#outputs[0].shape #torch.Size([1, 8, 768])\n",
        "#outputs[1].shape #torch.Size([1, 768])\n",
        "#len(outputs[2]) #tuple of 13 elements each of torch.Size([1, 768])\n",
        "#len(outputs[3]) #tuple of 12 elements each of torch.Size([1, 12, 8, 8])\n",
        "\n",
        "#format of output: ((tensor),(tensor),((tensor)*13),((tensor)*12))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akg-2sSChGA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train fct\n",
        "\n",
        "def train_model(train_loader, optimizer, criterion, scheduler,  config):\n",
        "  model.train()\n",
        "  avg_losst = 0\n",
        "  avg_loss1 = 0\n",
        "  avg_loss2 = 0\n",
        "  avg_loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  for idx, batch in enumerate(train_loader):\n",
        "    input_ids, input_masks, input_segments, labels, _ = batch\n",
        "    input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "    outputs = model(input_ids = input_ids.long(), attention_mask = input_masks, token_type_ids = input_segments, labels=None)\n",
        "    logits = outputs[0]\n",
        "    tot_loss = criterion(logits, labels)\n",
        "    q_loss = criterion(logits[:,:21], labels[:,:21])\n",
        "    a_loss = criterion(logits[:,21:], labels[:,21:])\n",
        "    loss = config.question_weight*q_loss + config.answer_weight*a_loss\n",
        "    loss.backward()\n",
        "    if (idx+1) % config.accum_steps ==0:\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    avg_losst += tot_loss.item()/len(train_loader)\n",
        "    avg_loss1 += q_loss.item()/len(train_loader)\n",
        "    avg_loss2 += a_loss.item()/len(train_loader)\n",
        "    avg_loss += loss.item()/len(train_loader)\n",
        "    del input_ids, input_masks, input_segments, labels\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return avg_losst, avg_loss1, avg_loss2, avg_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO97Y4XhrPYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_model(val_loader, val_shape, batch_size):\n",
        "  avg_val_loss = 0\n",
        "  model.eval()\n",
        "  val_preds = np.zeros((val_shape,len(target_cols)))  \n",
        "  originals = np.zeros((val_shape,len(target_cols)))\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(val_loader):\n",
        "      input_ids, input_masks, input_segments, labels, _ = batch\n",
        "      input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)\n",
        "      val_outputs = model(input_ids = input_ids.long(), attention_mask = input_masks, token_type_ids = input_segments, labels=None)\n",
        "      logits = val_outputs[0]\n",
        "      avg_val_loss += criterion(logits, labels).item()/len(val_loader)\n",
        "      val_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n",
        "      originals[idx*batch_size : (idx+1)*batch_size] = labels.detach().cpu().squeeze().numpy()\n",
        "\n",
        "    score = 0\n",
        "    preds = torch.sigmoid(torch.tensor(val_preds)).numpy()\n",
        "    rho_val = np.mean([spearmanr(originals[:,i],preds[:,i]).correlation for i in range(preds.shape[1])])\n",
        "    print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n",
        "  return avg_val_loss, rho_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnlp0wWgr78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "\n",
        "def predict_result(model, test_loader, batch_size):\n",
        "  model.eval()\n",
        "  preds = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "  tz = tqdm(enumerate(test_loader))\n",
        "  for idx, batch in tz:\n",
        "    with torch.no_grad():\n",
        "      test_out = model(input_ids = batch[0].to(device), attention_mask = batch[1].to(device), labels = None, token_type_ids = batch[2].to(device))\n",
        "      test_out = test_out[0]\n",
        "      preds[idx*batch_size : (idx+1)*batch_size]  = test_out.detach().cpu().squeeze().numpy()\n",
        "  \n",
        "  output  = torch.sigmoid(torch.tensor(preds)).numpy()\n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdFHxEirFwL",
        "colab_type": "code",
        "outputId": "7808a05b-f3a1-4fba-9137-0a2006daec3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7fe09f5774e0>\n",
            "1\n",
            "Tesla P4\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN0d_1iDj09X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare inputs for model training\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_categories = list(train.columns[[1,2,5]])\n",
        "input_categories\n",
        "\n",
        "bert_config = BertConfig.from_json_file('./datasets/transformers/bert-base-uncased-config.json')\n",
        "bert_config.num_labels = len(target_cols)\n",
        "bert_config.output_attentions = True\n",
        "bert_config.output_hidden_states = True\n",
        "\n",
        "bert_model = 'bert-base-uncased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "output_model_file = 'plain_pytorch_model1.bin'\n",
        "\n",
        "test_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239)\n",
        "lengths_test = np.argmax(test_inputs[0] == 0, axis=1)\n",
        "lengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n",
        "\n",
        "\n",
        "#print(results.shape)\n",
        "#print(len(test_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ17g5u9sXNx",
        "colab_type": "code",
        "outputId": "01858e33-f572-4d8a-fa88-06d008864e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# Train the model and evaluate\n",
        "\n",
        "num_folds = config.fold\n",
        "seed  = config.seed\n",
        "batch_size = 4\n",
        "epochs = config.epochs\n",
        "accum_steps  = 1\n",
        "result = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "test_set = QuestDataset(test_inputs, lengths_test, labels=None)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "kf = MultilabelStratifiedKFold(n_splits = num_folds, random_state = seed)\n",
        "y_train = train[target_cols].values\n",
        "\n",
        "if config.train:\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(train.values, y_train)):\n",
        "    if fold>0:\n",
        "      break\n",
        "    print(\"Current fold is : \", fold)\n",
        "    \n",
        "    train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "    print(\"train shape is %s and val shape is %s\" % (train_df.shape, val_df.shape))\n",
        "    \n",
        "    inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_train = compute_output_arrays(train_df, columns=target_cols)\n",
        "    outputs_train = torch.tensor(outputs_train,dtype=torch.float32)\n",
        "    lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n",
        "    lengths_train[lengths_train == 0] = inputs_train[0].shape[1] \n",
        "    train_set = QuestDataset(inputs_train, lengths_train, labels = outputs_train)\n",
        "    train_sampler = RandomSampler(train_set)\n",
        "    train_loader = DataLoader(train_set, batch_size = batch_size, sampler=train_sampler)\n",
        "\n",
        "    inputs_val = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239,a_max_len=239)\n",
        "    outputs_val = compute_output_arrays(val_df, columns=target_cols)\n",
        "    outputs_val = torch.tensor(outputs_val,dtype=torch.float32)\n",
        "    lengths_val = np.argmax(inputs_val[0] == 0, axis=1)\n",
        "    lengths_val[lengths_val == 0] = inputs_val[0].shape[1] \n",
        "    val_set = QuestDataset(inputs_val, lengths_val, labels = outputs_val)\n",
        "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = CustomBert.from_pretrained(bert_model, config = bert_config)\n",
        "    model.zero_grad()\n",
        "    model.to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if config.freeze:\n",
        "      for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.train()\n",
        "\n",
        "    i=0\n",
        "    best_avg_loss = 100\n",
        "    best_score = -1\n",
        "    best_param_loss = None\n",
        "    best_param_score = None\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [{'params':[p for n,p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0.8},{'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                     'weight_decay': 0}]\n",
        "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr = config.lr, eps = 4e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps=epochs*len(train_loader)//accum_steps)\n",
        "    print(\"##### Training ######\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      print(\"fold # {} and epoch # {}\".format(fold,epoch))\n",
        "      torch.cuda.empty_cache()\n",
        "      start_time = time.time()\n",
        "      avg_losst, avg_loss1, avg_loss2, avg_loss = train_model(train_loader, optimizer, criterion, scheduler, config)\n",
        "      avg_val_loss, score = val_model(val_loader, val_df.shape[0], batch_size=batch_size)\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(\"Epoch {}/{} \\t train_loss_overall = {:.4f} \\t train_loss = {:.4f} \\t train_loss_Question = {:.4f} \\t train_loss_Answer = {:.4f} \\t val_loss_overall = {:.4f} \\t score={:.6f} \\t time = {:.2f}s\".format(epoch+1, epochs, avg_losst,avg_loss, avg_loss1, avg_loss2, avg_val_loss,score, elapsed_time))\n",
        "      if best_avg_loss>avg_val_loss:\n",
        "        i=0\n",
        "        best_avg_loss = avg_val_loss\n",
        "        best_param_loss = model.state_dict()\n",
        "\n",
        "      if best_score < score:\n",
        "        best_score = score\n",
        "        best_param_score = model.state_dict()\n",
        "        print(\"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "        torch.save(best_param_score, \"best_param_score_{}_{}.pt\".format(config.expname, fold+1))\n",
        "      else:\n",
        "        i=i+1\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    del model\n",
        "    model = CustomBert.from_pretrained(bert_model, config = bert_config)\n",
        "    model.zero_grad()\n",
        "    model.to(device)\n",
        "    torch.cuda.empty_cache()\n",
        "    model.load_state_dict(best_param_score)\n",
        "    result += predict_result(model, test_loader, batch_size=batch_size)/num_folds\n",
        "\n",
        "  del train_df, val_df, model, optimizer, scheduler, criterion, val_loader, train_loader, test_loader, val_set, train_set\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()   \n",
        "\n",
        "\n",
        "\n",
        "print(\"Final result: %s\" %(result.shape))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current fold is :  0\n",
            "train shape is (4866, 41) and val shape is (1213, 41)\n",
            "##### Training ######\n",
            "fold # 0 and epoch # 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JKlXXw5BgvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#submission = pd.read_csv(r'../input/google-quest-challenge/sample_submission.csv')\n",
        "#submission.loc[:, 'question_asker_intent_understanding':] = result\n",
        "#submission.loc[~submission['qa_id'].isin(qa_id_list),'question_type_spelling']=0.0\n",
        "#submission.loc[submission['qa_id'].isin(qa_id_list),'question_type_spelling'] = 1.0\n",
        "\n",
        "#submission.to_csv('submission.csv', index=False)\n",
        "#submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od-FhRaL_P-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}